{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing VBD Outputs: Waymax vs GPUDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 15:00:06.193691: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742324406.212198   98476 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742324406.217715   98476 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-18 15:00:06.236913: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import waymax\n",
    "import numpy as np\n",
    "import math\n",
    "import mediapy\n",
    "from tqdm import tqdm\n",
    "import dataclasses\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from waymax import config as _config\n",
    "from waymax import dataloader, datatypes, visualization, dynamics\n",
    "from waymax.datatypes.simulator_state import SimulatorState\n",
    "from waymax.config import EnvironmentConfig, ObjectType\n",
    "\n",
    "# Set working directory to the base directory 'gpudrive'\n",
    "working_dir = Path.cwd()\n",
    "while working_dir.name != 'gpudrive':\n",
    "    working_dir = working_dir.parent\n",
    "    if working_dir == Path.home():\n",
    "        raise FileNotFoundError(\"Base directory 'gpudrive' not found\")\n",
    "os.chdir(working_dir)\n",
    "\n",
    "# VBD dependencies\n",
    "from integrations.models.vbd.sim_agent.waymax_env import WaymaxEnvironment\n",
    "from integrations.models.vbd.data.dataset import WaymaxTestDataset\n",
    "from integrations.models.vbd.waymax_visualization.plotting import plot_state\n",
    "from integrations.models.vbd.sim_agent.sim_actor import VBDTest, sample_to_action\n",
    "from integrations.models.vbd.model.utils import set_seed\n",
    "\n",
    "# GPUDrive dependencies\n",
    "import gpudrive\n",
    "from gpudrive.env.config import EnvConfig, RenderConfig, SceneConfig, SelectionDiscipline\n",
    "from gpudrive.env.env_torch import GPUDriveTorchEnv\n",
    "from gpudrive.env.dataset import SceneDataLoader\n",
    "from gpudrive.visualize.utils import img_from_fig\n",
    "\n",
    "# Plotting\n",
    "sns.set(\"notebook\")\n",
    "sns.set_style(\"ticks\", rc={\"figure.facecolor\": \"none\", \"axes.facecolor\": \"none\"})\n",
    "#%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "# Ignore all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'data/processed' # Base data path\n",
    "CKPT_PATH = 'integrations/models/vbd/weights/epoch=18.ckpt'\n",
    "\n",
    "SCENARIO_ID = 'efc5cbe01b4a526f'\n",
    "\n",
    "FPS = 20\n",
    "INIT_STEPS = 11 # Warmup period\n",
    "MAX_CONTROLLED_OBJECTS = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained VBD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = VBDTest.load_from_checkpoint(CKPT_PATH, torch.device('cpu'))\n",
    "_ = model.cpu()\n",
    "_ = model.eval();\n",
    "\n",
    "# Model settings\n",
    "replan_freq=80 # Roll out every X steps 80 means openloop\n",
    "model.early_stop=0 # Stop Diffusion Early From 100 to X\n",
    "model.skip = 1 # Skip Alpha \n",
    "model.reward_func = None\n",
    "\n",
    "# Ensure reproducability\n",
    "#set_seed(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_side_by_side_gif(frames1, frames2, output_path, fps=FPS):\n",
    "    \"\"\"\n",
    "    Saves two arrays of frames as a single side-by-side GIF.\n",
    "    The second frame array is resized to match the dimensions of the first frame array.\n",
    "    \n",
    "    Parameters:\n",
    "    frames1 (list of np.ndarray): First array of frames (images).\n",
    "    frames2 (list of np.ndarray): Second array of frames (images).\n",
    "    output_path (str): Path to save the output GIF.\n",
    "    fps (int): Frames per second for the GIF.\n",
    "    \"\"\"\n",
    "    # Ensure both arrays have the same number of frames\n",
    "    if len(frames1) != len(frames2):\n",
    "        raise ValueError(\"The two frame arrays must have the same number of frames.\")\n",
    "    \n",
    "    # Combine frames side by side\n",
    "    combined_frames = []\n",
    "    for frame1, frame2 in zip(frames1, frames2):\n",
    "        # Get the dimensions of frame1\n",
    "        height1, width1 = frame1.shape[:2]\n",
    "\n",
    "        if frame2.shape[0] != height1 or frame2.shape[1] != width1:\n",
    "            # Resize frame2 to match the dimensions of frame1\n",
    "            pil_image = Image.fromarray(frame2)\n",
    "            frame2 = np.array(pil_image.resize((width1, height1)))\n",
    "        \n",
    "        # Concatenate frames horizontally\n",
    "        combined_frame = np.hstack((frame1, frame2))\n",
    "        combined_frames.append(combined_frame)\n",
    "    \n",
    "    # Save the combined frames as a GIF\n",
    "    mediapy.write_video(output_path, combined_frames, fps=fps, codec=\"gif\")\n",
    "    print(f\"GIF saved at {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 52 scenarios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Diffusion:   2%|▏         | 1/50 [00:00<00:33,  1.47it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 45\u001b[0m\n\u001b[1;32m     38\u001b[0m     sample \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mprocess_scenario(\n\u001b[1;32m     39\u001b[0m     current_state, \n\u001b[1;32m     40\u001b[0m     current_index \u001b[38;5;241m=\u001b[39m current_state\u001b[38;5;241m.\u001b[39mtimestep,\n\u001b[1;32m     41\u001b[0m     use_log\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     42\u001b[0m     selected_agents\u001b[38;5;241m=\u001b[39mselected_agents, \u001b[38;5;66;03m# override the agent selection by distance to the ego\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     )\n\u001b[1;32m     44\u001b[0m     batch \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39m__collate_fn__([sample])\n\u001b[0;32m---> 45\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_denoiser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     traj_pred \u001b[38;5;241m=\u001b[39m pred[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdenoised_trajs\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Get action\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gpudrive/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Projects/gpudrive/integrations/models/vbd/sim_agent/sim_actor.py:636\u001b[0m, in \u001b[0;36mVBDTest.sample_denoiser\u001b[0;34m(self, batch, num_samples, x_t, use_tqdm, global_frame, **kwargs)\u001b[0m\n\u001b[1;32m    634\u001b[0m     guide_history\u001b[38;5;241m.\u001b[39mappend(guide)\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 636\u001b[0m     denoiser_output, x_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_denoiser\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_t\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mglobal_frame\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_frame\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    642\u001b[0m     guide \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    644\u001b[0m denoiser_output_history\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    645\u001b[0m     torch_dict_to_numpy(denoiser_output)\n\u001b[1;32m    646\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/Projects/gpudrive/integrations/models/vbd/sim_agent/sim_actor.py:542\u001b[0m, in \u001b[0;36mVBDTest.step_denoiser\u001b[0;34m(self, x_t, c, t, global_frame)\u001b[0m\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDenoiser is not defined\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    541\u001b[0m \u001b[38;5;66;03m# Denoise to reconstruct x_0 ~ D(x_t, c, t)\u001b[39;00m\n\u001b[0;32m--> 542\u001b[0m denoiser_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_denoiser\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnoised_actions_normalized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdiffusion_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m    \u001b[49m\u001b[43mglobal_frame\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_frame\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    549\u001b[0m x_0 \u001b[38;5;241m=\u001b[39m denoiser_output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdenoised_actions_normalized\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    551\u001b[0m \u001b[38;5;66;03m# Step to sample from P(x_t-1 | x_t, x_0)\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Projects/gpudrive/integrations/models/vbd/model/VBD.py:177\u001b[0m, in \u001b[0;36mVBD.forward_denoiser\u001b[0;34m(self, encoder_outputs, noised_actions_normalized, diffusion_step, global_frame)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124;03mForward pass of the denoiser module.\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m    denoiser_outputs: Dictionary containing the denoiser outputs.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    176\u001b[0m noised_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnormalize_actions(noised_actions_normalized)\n\u001b[0;32m--> 177\u001b[0m denoised_actions_normalized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdenoiser\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoised_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiffusion_step\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m current_states \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magents\u001b[39m\u001b[38;5;124m\"\u001b[39m][:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agents_len, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    182\u001b[0m     encoder_outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magents\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agents_len\n\u001b[1;32m    183\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToo many agents to consider\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/gpudrive/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gpudrive/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Projects/gpudrive/integrations/models/vbd/model/modules.py:205\u001b[0m, in \u001b[0;36mDenoiser.forward\u001b[0;34m(self, encoder_inputs, noisy_actions, diffusion_step)\u001b[0m\n\u001b[1;32m    197\u001b[0m noise_level \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnoise_level_embedding(diffusion_step)\n\u001b[1;32m    198\u001b[0m noisy_states_local \u001b[38;5;241m=\u001b[39m roll_out(\n\u001b[1;32m    199\u001b[0m     current_states,\n\u001b[1;32m    200\u001b[0m     noisy_actions,\n\u001b[1;32m    201\u001b[0m     action_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_action_len,\n\u001b[1;32m    202\u001b[0m     global_frame\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    203\u001b[0m )\n\u001b[0;32m--> 205\u001b[0m denoised_actions_normalized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnoisy_states_local\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencodings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m denoised_actions_normalized\n",
      "File \u001b[0;32m~/miniconda3/envs/gpudrive/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gpudrive/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Projects/gpudrive/integrations/models/vbd/model/modules.py:602\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[0;34m(self, noisy_trajectories, noise_level, encodings, relations, mask)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agents_len):\n\u001b[1;32m    592\u001b[0m     query_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_layers[\u001b[38;5;241m2\u001b[39m](\n\u001b[1;32m    593\u001b[0m         query_content_stack[:, i],\n\u001b[1;32m    594\u001b[0m         query_content_stack\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    600\u001b[0m         attn_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcasual_mask[i],\n\u001b[1;32m    601\u001b[0m     )  \u001b[38;5;66;03m# [B, T, 256]\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m     query_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_layers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_content\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencodings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelations\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [B, T, 256]\u001b[39;00m\n\u001b[1;32m    605\u001b[0m     query_content_list\u001b[38;5;241m.\u001b[39mappend(query_content)\n\u001b[1;32m    607\u001b[0m query_content_stack \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(\n\u001b[1;32m    608\u001b[0m     query_content_list, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    609\u001b[0m )  \u001b[38;5;66;03m# [B, Na, T, 256]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gpudrive/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gpudrive/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Projects/gpudrive/integrations/models/vbd/model/modules.py:501\u001b[0m, in \u001b[0;36mCrossTransformer.forward\u001b[0;34m(self, query, key, relations, attn_mask, key_mask)\u001b[0m\n\u001b[1;32m    498\u001b[0m     attention_output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attention(query, key, value)\n\u001b[1;32m    500\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_1(attention_output)\n\u001b[0;32m--> 501\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_2(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mffn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m attention_output)\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/envs/gpudrive/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gpudrive/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gpudrive/lib/python3.11/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/gpudrive/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gpudrive/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gpudrive/lib/python3.11/site-packages/torch/nn/modules/dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gpudrive/lib/python3.11/site-packages/torch/nn/functional.py:1295\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m(\u001b[38;5;28minput\u001b[39m, p, training)\n",
      "File \u001b[0;32m~/miniconda3/envs/gpudrive/lib/python3.11/site-packages/torch/_VF.py:26\u001b[0m, in \u001b[0;36mVFModule.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(name)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_VariableFunctions\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvf, attr)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create waymax test \"dataset\" obj (need for utils)\n",
    "dataset = WaymaxTestDataset(\n",
    "    data_dir = 'data/processed/debug/waymax', \n",
    "    anchor_path = 'integrations/models/vbd/data/cluster_64_center_dict.pkl',\n",
    "    max_object=MAX_CONTROLLED_OBJECTS,\n",
    ")\n",
    "#Init Waymax env\n",
    "env_config = EnvironmentConfig(\n",
    "    controlled_object=ObjectType.VALID,\n",
    "    allow_new_objects_after_warmup=False,\n",
    "    init_steps=INIT_STEPS+1,\n",
    "    max_num_objects=MAX_CONTROLLED_OBJECTS,\n",
    ")\n",
    "waymax_env = WaymaxEnvironment(\n",
    "    dynamics_model=dynamics.StateDynamics(),\n",
    "    config=env_config,\n",
    "    log_replay = True,\n",
    ")\n",
    "\n",
    "with open(f'{DATA_DIR}/debug/waymax/waymax_scenario_{SCENARIO_ID}.pkl', 'rb') as f:\n",
    "    scenario = pickle.load(f)\n",
    "\n",
    "#Generate video with vbd trajectories\n",
    "init_state = waymax_env.reset(scenario)\n",
    "current_state = init_state\n",
    "sample = dataset.process_scenario(\n",
    "init_state,\n",
    "current_index=init_state.timestep,\n",
    "use_log=False\n",
    ")\n",
    "is_controlled = sample['agents_interested'] > 0\n",
    "selected_agents = sample['agents_id'][is_controlled]\n",
    "state_logs = [current_state]\n",
    "\n",
    "for i in range(current_state.remaining_timesteps):\n",
    "    t = i % replan_freq\n",
    "    if t == 0:\n",
    "        sample = dataset.process_scenario(\n",
    "        current_state, \n",
    "        current_index = current_state.timestep,\n",
    "        use_log=False,\n",
    "        selected_agents=selected_agents, # override the agent selection by distance to the ego\n",
    "        )\n",
    "        batch = dataset.__collate_fn__([sample])\n",
    "        pred = model.sample_denoiser(batch)\n",
    "        traj_pred = pred['denoised_trajs'].cpu().numpy()[0]\n",
    "\n",
    "    # Get action\n",
    "    action_sample = traj_pred[:, t, :]\n",
    "    action = sample_to_action(\n",
    "    action_sample, \n",
    "    is_controlled, \n",
    "    agents_id=selected_agents, \n",
    "    max_num_objects=MAX_CONTROLLED_OBJECTS\n",
    "    )\n",
    "    # Step the simulator\n",
    "    current_state = waymax_env.step_sim_agent(current_state, [action])\n",
    "    state_logs.append(current_state)\n",
    "\n",
    "waymax_frames = [plot_state(state) for state in state_logs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Diffusion: 100%|██████████| 50/50 [00:18<00:00,  2.64it/s]\n"
     ]
    }
   ],
   "source": [
    "#Init GPUDrive env\n",
    "env_config = EnvConfig(\n",
    "    init_steps=INIT_STEPS, # Warmup period\n",
    "    remove_non_vehicles=False, # Control vehicles, pedestrians, and cyclists\n",
    "    dynamics_model=\"state\", # Use state-based dynamics model\n",
    "    dist_to_goal_threshold=1e-5, # Trick to make sure the agents don't disappear when they reach the goal\n",
    "    collision_behavior=\"ignore\", # Ignore collisions\n",
    "    init_mode = 'all_non_trivial',\n",
    "    use_vbd=True,\n",
    "    max_controlled_agents=32,\n",
    "    vbd_model_path=CKPT_PATH,\n",
    ")\n",
    "        \n",
    "scene_config = SceneConfig(batch_size=1, dataset_size=1, path=\"data/processed/temp\", num_scenes=1)\n",
    "# Make env\n",
    "gpudrive_env = GPUDriveTorchEnv(\n",
    "    config=env_config,\n",
    "    data_loader = SceneDataLoader(\n",
    "        root=\"data/processed/temp\",\n",
    "        batch_size=scene_config.batch_size,\n",
    "        dataset_size=scene_config.dataset_size,\n",
    "    ),\n",
    "    render_config=RenderConfig(resolution=(400, 400)),\n",
    "    max_cont_agents=MAX_CONTROLLED_OBJECTS, # Maximum number of agents to control per scene\n",
    "    device=\"cpu\",\n",
    ")\n",
    "gpudrive_sample_batch = gpudrive_env._generate_sample_batch()\n",
    "\n",
    "# Reset predictions tensor\n",
    "pred_trajs = torch.zeros((gpudrive_env.num_worlds, gpudrive_env.max_agent_count, env_config.episode_len-INIT_STEPS, 10))\n",
    "\n",
    "# Fill pred_trajs correctly for each world\n",
    "for i in range(gpudrive_env.num_worlds):\n",
    "    world_agent_indices = gpudrive_sample_batch['agents_id'][i]\n",
    "    \n",
    "    # Filter out negative indices (padding values)\n",
    "    valid_mask = world_agent_indices >= 0  # Boolean mask of valid indices\n",
    "    valid_agent_indices = world_agent_indices[valid_mask]  # Filtered tensor\n",
    "\n",
    "    # Use tensor indexing with valid agent indices\n",
    "    pred_trajs[i, valid_agent_indices, :, :2] = gpudrive_env.vbd_trajectories[i, valid_agent_indices, :, :2]  # pos x, y\n",
    "    pred_trajs[i, valid_agent_indices, :, 3] = gpudrive_env.vbd_trajectories[i, valid_agent_indices, :, 2]    # yaw\n",
    "    pred_trajs[i, valid_agent_indices, :, 4:6] = gpudrive_env.vbd_trajectories[i, valid_agent_indices, :, 3:5]  # vel x, y\n",
    "\n",
    "# Now step through the simulation\n",
    "gpudrive_frames = []\n",
    "for t in range(env_config.episode_len-INIT_STEPS):\n",
    "    gpudrive_env.step_dynamics(pred_trajs[:, :, t, :])\n",
    "    fig = gpudrive_env.vis.plot_simulator_state(\n",
    "        time_steps=[t],\n",
    "        env_indices=[0],\n",
    "        zoom_radius=70,\n",
    "    )[0]\n",
    "    gpudrive_frames.append(img_from_fig(fig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIF saved at integrations/models/videos/train.gif\n"
     ]
    }
   ],
   "source": [
    "mediapy.write_video('integrations/models/videos/train.gif', gpudrive_frames, fps=FPS, codec=\"gif\")\n",
    "print(\"GIF saved at integrations/models/videos/train.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 80, 10]) torch.Size([1, 64, 80, 5])\n"
     ]
    }
   ],
   "source": [
    "print(pred_trajs.shape, gpudrive_env.vbd_trajectories.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2,  4,  6, 21, 24, 25, 32, 33, 34, 35, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "       dtype=torch.int32)\n",
      "tensor([ 2,  4,  6, 21, 24, 25, 32, 33, 34, 35], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "world_agent_indices = gpudrive_sample_batch['agents_id'][0]\n",
    "valid_mask = world_agent_indices >= 0  # Boolean mask of valid indices\n",
    "valid_agent_indices = world_agent_indices[valid_mask]  # Filtered tensor\n",
    "\n",
    "print(world_agent_indices)\n",
    "print(valid_agent_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIF saved at integrations/models/videos/efc5cbe01b4a526f.gif\n"
     ]
    }
   ],
   "source": [
    "# Save gif for comparison\n",
    "save_side_by_side_gif(waymax_frames, gpudrive_frames, f'integrations/models/videos/{SCENARIO_ID}.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting VBD trajectory as part of agent obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_ego_frame(trajectory: torch.Tensor, ego_pos: torch.Tensor, ego_yaw: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Transform trajectory from global coordinates to ego-centric frame.\n",
    "    Args:\n",
    "    trajectory: Shape (time_steps, 2) containing x,y coordinates in global frame\n",
    "    ego_pos: Shape (2,) containing ego x,y position\n",
    "    ego_yaw: Shape (1,) containing ego yaw angle in radians\n",
    "    Returns:\n",
    "    transformed_trajectory: Shape (time_steps, 2) in ego-centric frame\n",
    "    \"\"\"\n",
    "    # Step 1: Translate trajectory to be relative to ego position\n",
    "    translated = trajectory - ego_pos\n",
    "    \n",
    "    # Step 2: Rotate trajectory to align with ego orientation\n",
    "    # Create rotation matrix\n",
    "    cos_yaw = torch.cos(ego_yaw)\n",
    "    sin_yaw = torch.sin(ego_yaw)\n",
    "    rotation_matrix = torch.tensor([\n",
    "        [cos_yaw, sin_yaw],\n",
    "        [-sin_yaw, cos_yaw]\n",
    "    ])\n",
    "    \n",
    "    # Apply rotation matrix to the translated trajectory\n",
    "    # We need to transpose rotation_matrix for batch matrix multiplication\n",
    "    transformed_trajectory = torch.matmul(translated, rotation_matrix.T)\n",
    "    \n",
    "    return transformed_trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting vbd trajectory as part of observation\n",
    "from gpudrive.datatypes.observation import GlobalEgoState\n",
    "init_state = gpudrive_env.reset()\n",
    "# Get global agent observations\n",
    "global_agent_obs = GlobalEgoState.from_tensor(\n",
    "    abs_self_obs_tensor=gpudrive_env.sim.absolute_self_observation_tensor(),\n",
    "    backend=gpudrive_env.backend,\n",
    "    device=gpudrive_env.device,\n",
    ")\n",
    "\n",
    "for agent_index in valid_agent_indices:\n",
    "    vbd_trajectory = gpudrive_env.vbd_trajectories[0, agent_index, :, :2]\n",
    "    pos_xy = torch.tensor([global_agent_obs.pos_x[0, agent_index], global_agent_obs.pos_y[0, agent_index]])\n",
    "    yaw = torch.tensor(global_agent_obs.rotation_angle[0, agent_index])\n",
    "\n",
    "    transformed_trajectory = transform_to_ego_frame(vbd_trajectory, pos_xy, yaw)\n",
    "\n",
    "    fig = gpudrive_env.vis.plot_agent_observation(\n",
    "        agent_idx=agent_index,\n",
    "        env_idx=0,\n",
    "        trajectory=transformed_trajectory,\n",
    "        figsize = (4, 4)\n",
    "    )\n",
    "    fig.savefig(f'integrations/models/videos/{agent_index}_vbd_trajectory.png', facecolor='white', transparent=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpudrive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
