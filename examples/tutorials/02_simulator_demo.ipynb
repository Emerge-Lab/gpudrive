{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `GPUDrive` simulator concepts\n",
    "\n",
    "In this notebook, we demonstrate how to work with the `GPUDrive` simulator and access its basic attributes in Python. The simulator, written in C++, is built on top of the [Madrona Engine](https://madrona-engine.github.io/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import gpudrive\n",
    "\n",
    "# Set working directory to the base directory 'gpudrive'\n",
    "working_dir = Path.cwd()\n",
    "while working_dir.name != 'gpudrive':\n",
    "    working_dir = working_dir.parent\n",
    "    if working_dir == Path.home():\n",
    "        raise FileNotFoundError(\"Base directory 'gpudrive' not found\")\n",
    "os.chdir(working_dir)\n",
    "\n",
    "\n",
    "from pygpudrive.env.config import SceneConfig\n",
    "from pygpudrive.env.scene_selector import select_scenes\n",
    "scene_config = SceneConfig(path=\"data\", num_scenes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "- `GPUDrive` simulations are discretized traffic scenarios. A scenario is a constructed snapshot of traffic situation at a particular timepoint.\n",
    "- The state of the vehicle of focus is referred to as the **ego state**. Each vehicle has their own partial view of the traffic scene; and a visible state is constructed by parameterizing the view distance of the driver. The **action** for each vehicle is a (1, 3) tuple with the acceleration, steering and head angle of the vehicle.\n",
    "- The `step()` method advances the simulation with a desired step size. By default, the dynamics of vehicles are driven by a kinematic bicycle model. If a vehicle is not controlled (that is, we do not give it actions), its position, heading, and speed will be updated according to a the human expert demonstrations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating a sim object with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Ratio unique scenes / number of worls = 1 / 1 ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sim = gpudrive.SimManager(\n",
    "    exec_mode=gpudrive.madrona.ExecMode.CUDA\n",
    "    if device == \"cuda\"\n",
    "    else gpudrive.madrona.ExecMode.CPU,\n",
    "    gpu_id=0,\n",
    "    scenes=select_scenes(scene_config),\n",
    "    params=gpudrive.Parameters(),  # Environment parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simulator provides the following functions:\n",
    "- `reset(world_idx)` resets a specific world or environment at the given index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.reset([0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `step()` advances the dynamics of all worlds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting tensors\n",
    "\n",
    "To retrieve a tensor from the simulator, call the specific `tensor()` method, followed by either `to_torch()` or `to_jax()`.\n",
    "\n",
    "For example, here is how to access the ego state, or self-observation tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 128, 6]), device(type='cpu'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_tensor = sim.self_observation_tensor().to_torch()\n",
    "\n",
    "observation_tensor.shape, observation_tensor.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or alternatively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 128, 6), {CpuDevice(id=0)})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_tensor_jax = sim.self_observation_tensor().to_jax()\n",
    "\n",
    "observation_tensor_jax.shape, observation_tensor_jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are all available tensor exports and methods on the sim object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absolute_self_observation_tensor\n",
      "action_tensor\n",
      "agent_roadmap_tensor\n",
      "controlled_state_tensor\n",
      "depth_tensor\n",
      "done_tensor\n",
      "expert_trajectory_tensor\n",
      "info_tensor\n",
      "lidar_tensor\n",
      "map_observation_tensor\n",
      "partner_observations_tensor\n",
      "reset\n",
      "response_type_tensor\n",
      "reward_tensor\n",
      "rgb_tensor\n",
      "self_observation_tensor\n",
      "shape_tensor\n",
      "step\n",
      "steps_remaining_tensor\n",
      "valid_state_tensor\n"
     ]
    }
   ],
   "source": [
    "for attr in dir(sim):\n",
    "    if not attr.startswith(\"_\"):\n",
    "        print(attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect valid and controlled agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the number of agents and road points in each world, you can use the `shape_tensor`.\n",
    "\n",
    "The shape tensor is a 2D tensor where the first dimension represents the number of worlds, and the second dimension represents the shape of each world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape tensor has a shape of (Num Worlds, 2): (1, 2)\n",
      "World 0 has 10 VALID agents and 3195 VALID road objects\n"
     ]
    }
   ],
   "source": [
    "shape_tensor = sim.shape_tensor().to_jax()\n",
    "print(f\"Shape tensor has a shape of (Num Worlds, 2): {shape_tensor.shape}\")\n",
    "\n",
    "for world_idx in range(shape_tensor.shape[0]):\n",
    "    print(\n",
    "        f\"World {world_idx} has {shape_tensor[world_idx][0]} VALID agents and {shape_tensor[world_idx][1]} VALID road objects\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of **valid** agents indicates the number of controllable agents (vehicles). Some vehicles or bicycles may be initialized in incorrect positions or remain static; these are marked as **invalid** and cannot be controlled.\n",
    "\n",
    "The sim comes with a mask that indicates which agents can be controlled. Entries are `1` for agents that can be controlled, and `0` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Controlled state tensor has a shape of (num_worlds, max_num_agents_in_scene, 1):  torch.Size([1, 128, 1])\n"
     ]
    }
   ],
   "source": [
    "controlled_state_tensor = sim.controlled_state_tensor().to_torch()\n",
    "print(\n",
    "    \"Controlled state tensor has a shape of (num_worlds, max_num_agents_in_scene, 1): \",\n",
    "    controlled_state_tensor.shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can control 3 agents in this world\n",
    "controlled_state_tensor.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "controlled_state_tensor.sum().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions\n",
    "\n",
    "The action space consists of three types:\n",
    "\n",
    "- **Acceleration**: Continuous float values representing the acceleration applied to the agents. This affects how quickly an agent speeds up or slows down.\n",
    "- **Steering Angle**: Continuous float values representing the steering angle, following the bicycle kinematic model. This determines how sharply an agent turns.\n",
    "- **Heading Angle** (currently unused): Continuous float values for the heading angle, which control the direction an agent is facing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action tensor stores the current actions for all agents across all worlds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action tensor has a shape of (num_worlds, max_num_agents_in_scene, 3): torch.Size([1, 128, 3])\n"
     ]
    }
   ],
   "source": [
    "action_tensor = sim.action_tensor().to_torch()\n",
    "print(\n",
    "    f\"Action tensor has a shape of (num_worlds, max_num_agents_in_scene, 3): {action_tensor.shape}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set the actions for all controlled agents, we use the `copy_()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions tensor after setting all actions to 1: tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "actions_tensor = sim.action_tensor().to_torch()\n",
    "\n",
    "actions = torch.full(actions_tensor.shape, 1.0)\n",
    "actions_tensor.copy_(actions)\n",
    "\n",
    "print(f\"Actions tensor after setting all actions to 1: {actions_tensor[0][0]}\")\n",
    "\n",
    "# Call step() to apply the actions\n",
    "sim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the simulator settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "IgnoreNonVehicles   : False\n",
      "collisionBehaviour  : gpudrive.CollisionBehaviour.AgentStop\n",
      "disableClassicalObs : False\n",
      "enableLidar         : False\n",
      "initOnlyValidAgentsAtFirstStep : True\n",
      "isStaticAgentControlled: False\n",
      "maxNumControlledVehicles: 10000\n",
      "observationRadius   : 0.0\n",
      "polylineReductionThreshold: 0.0\n",
      "rewardParams        : <gpudrive.RewardParams object at 0x7f8808109130>\n",
      "Reward parameters:\n",
      "    distanceToExpertThreshold: 0.0\n",
      "    distanceToGoalThreshold: 0.0\n",
      "    rewardType        : gpudrive.RewardType.DistanceBased\n",
      "roadObservationAlgorithm: gpudrive.FindRoadObservationsWith.KNearestEntitiesWithRadiusFiltering\n",
      "useWayMaxModel      : False\n"
     ]
    }
   ],
   "source": [
    "params = gpudrive.Parameters()\n",
    "\n",
    "print(\"Parameters:\")\n",
    "for attr in dir(params):\n",
    "    if not attr.startswith(\"__\"):\n",
    "        value = getattr(params, attr)\n",
    "        print(f\"{attr:20}: {value}\")\n",
    "        if attr == \"rewardParams\":\n",
    "            print(\"Reward parameters:\")\n",
    "            reward_params = getattr(params, attr)\n",
    "            for attr2 in dir(reward_params):\n",
    "                if not attr2.startswith(\"__\"):\n",
    "                    value2 = getattr(reward_params, attr2)\n",
    "                    print(f\"    {attr2:18}: {value2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring the simulator "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set the parameters of the simulator, fill in the values for each attribute of the parameter object as below. This allows you to customize the simulation settings.\n",
    "\n",
    "The params object can be passed to the sim constructor like this:\n",
    "\n",
    "```Python\n",
    "sim = gpudrive.SimManager(\n",
    "    ...\n",
    "    params=params \n",
    ")\n",
    "```\n",
    "\n",
    "See our [README](https://github.com/Emerge-Lab/gpudrive/tree/main?tab=readme-ov-file#configuring-the-sim) for the full documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_params = gpudrive.RewardParams()\n",
    "reward_params.rewardType = gpudrive.RewardType.DistanceBased\n",
    "reward_params.distanceToGoalThreshold = 1.0\n",
    "reward_params.distanceToExpertThreshold = 1.0\n",
    "\n",
    "# Initialize Parameters\n",
    "params = gpudrive.Parameters()\n",
    "params.polylineReductionThreshold = 1.0\n",
    "params.observationRadius = 100.0\n",
    "params.collisionBehaviour = gpudrive.CollisionBehaviour.Ignore\n",
    "params.maxNumControlledVehicles = 10\n",
    "params.rewardParams = reward_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running an episode of the sim\n",
    "\n",
    "Putting everything together, the full interaction loop looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "reset(): incompatible function arguments. The following argument types are supported:\n    1. reset(self, arg: collections.abc.Sequence[int], /) -> None\n\nInvoked with types: gpudrive.SimManager, int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m actions_shape \u001b[38;5;241m=\u001b[39m sim\u001b[38;5;241m.\u001b[39maction_tensor()\u001b[38;5;241m.\u001b[39mto_torch()\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m      4\u001b[0m dones \u001b[38;5;241m=\u001b[39m sim\u001b[38;5;241m.\u001b[39mdone_tensor()\u001b[38;5;241m.\u001b[39mto_torch()\n",
      "\u001b[0;31mTypeError\u001b[0m: reset(): incompatible function arguments. The following argument types are supported:\n    1. reset(self, arg: collections.abc.Sequence[int], /) -> None\n\nInvoked with types: gpudrive.SimManager, int"
     ]
    }
   ],
   "source": [
    "sim.reset(0)\n",
    "\n",
    "actions_shape = sim.action_tensor().to_torch().shape\n",
    "dones = sim.done_tensor().to_torch()\n",
    "\n",
    "while not torch.all(sim.done_tensor().to_torch()):\n",
    "    obs, rews, dones = (\n",
    "        sim.self_observation_tensor().to_torch(),\n",
    "        sim.reward_tensor().to_torch(),\n",
    "        sim.done_tensor().to_torch(),\n",
    "    )\n",
    "    actions = torch.rand(actions_shape)\n",
    "    sim.action_tensor().to_torch().copy_(actions)\n",
    "    sim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [optional] Detailed documentation for simulator configurations 📜\n",
    "\n",
    "This section provides detailed information about the observation tensors, rewards, road reduction algorithm, collision behavior, and various other parameters used in the simulator.\n",
    "\n",
    "#### Observation Space\n",
    "\n",
    "**SelfObservation**\n",
    "\n",
    "The `SelfObservation` tensor of shape `(5,)` for each agent provides information about the agent's own state. The respective values are:\n",
    "\n",
    "- `SelfObservation[0]`: Represents the current *speed* of the agent.\n",
    "- `SelfObservation[1:3]`: *Length* and *width* of the agent.\n",
    "- `SelfObservation[3:5]`: *Coordinates (x,y)* of the goal relative to the agent.\n",
    "- `SelfObservation[5]`: Represents if the agent has collided. Values in `{0,1}`.\n",
    "\n",
    "**MapObservation**\n",
    "\n",
    "The `MapObservation` tensor of shape `(4,)` for each agent provides the *absolute* position of map objects. The values are:\n",
    "\n",
    "- `MapObservation[0:2]`: Represents the position of the `MapObject`.\n",
    "- `MapObservation[2:5]`: Represents the scale of the `MapObject` in terms of length, width, and height.\n",
    "- `MapObservation[5]`: Represents the heading angle of the `MapObject`.\n",
    "- `MapObservation[6]`: Represents the type of the `MapObject`.\n",
    "\n",
    "**PartnerObservation**\n",
    "\n",
    "The `PartnerObservation` tensor of shape `(num_agents-1, 7)` for each agent provides information about other agents within the `params.observationRadius`. All the values in this tensor are *relative to the ego agent*. The respective values for each `PartnerObservation` are:\n",
    "\n",
    "- `PartnerObservation[0]`: The *speed* of the observed neighboring agent.\n",
    "- `PartnerObservation[1:3]`: The *position (x,y)* of the observed neighboring agent.\n",
    "- `PartnerObservation[3]`: The *orientation* of the neighboring agent.\n",
    "- `PartnerObservation[4:6]`: The *length* and *width* of the neighboring agent.\n",
    "- `PartnerObservation[6]`: The type of agent.\n",
    "\n",
    "**AgentMapObservations**\n",
    "\n",
    "The `AgentMapObservations` tensor of shape `(num_road_objs, 7)` for each agent provides information about the road objects within the `params.observationRadius`. All the values in this tensor are *relative to the ego agent*. The respective values for each `AgentMapObservations` are the same as `MapObservations`.\n",
    "\n",
    "#### Rewards\n",
    "\n",
    "* `RewardType`: There are three types of rewards that can be exported from the simulator:\n",
    "  - `DistanceBased`: Exports the distance of the agent to the goal.\n",
    "  - `OnGoalAchieved`: Exports 1 if the agent has reached the goal, else 0.\n",
    "  - `Dense` (Not Implemented): Exports the distance of the agent from its expert trajectory specified in the dataset.\n",
    "* `distanceToGoalThreshold`: This threshold is used to determine if the agent has reached the goal or not. `Default: 0.0`.\n",
    "* `distanceToExpertThreshold`: This threshold is used to determine if the agent is following the expert trajectory or not. `Default: 0.0`.\n",
    "\n",
    "#### Road Reduction Algorithm\n",
    "\n",
    "To manage performance and simplify the observation space of the simulator, we apply a polyline reduction algorithm on the road edges, lines, and lanes. We use the ['Visvalingam-Whyatt Algorithm'](https://en.wikipedia.org/wiki/Visvalingam%E2%80%93Whyatt_algorithm).\n",
    "\n",
    "* `polylineReductionThreshold`: This threshold determines how much reduction is to be applied to the road lines. Ranges from `0` to `+ve inf`. If set to `0`, no reduction will be applied. `Default: 0.5`.\n",
    "\n",
    "#### Collision Behaviour\n",
    "\n",
    "For easy troubleshooting and learning various policies, the behavior of the agents on collisions can be configured.\n",
    "\n",
    "* `AgentStop`: The agents in collision would simply stop at the point of collision. No further actions will be applied to these agents.\n",
    "* `AgentRemoved`: The agents in collision would simply be removed from the scene.\n",
    "* `Ignore`: The agents in collision still output that they collided, but they will continue to move around as if they did not collide.\n",
    "\n",
    "#### Misc Params\n",
    "\n",
    "* `ObservationRadius`: Defines the radius within which an agent can observe its surroundings. Objects outside the `ObservationRadius` are zeroed out in the observations.\n",
    "* `MaxNumControlledVehicles`: Controls the maximum number of agents that can be controlled in the simulator. If a particular file has fewer valid agents, some worlds may have fewer controlled agents. We pick the first `MaxNumControlledVehicles` **valid** agents to control, and the rest are controlled via their expert trajectories.\n",
    "* `IgnoreNonVehicles`: Defines the policy of not initializing pedestrians/cyclists. Default: `false`.\n",
    "* `roadObservationAlgorithm`: Choose between `KNearestEntitiesWithRadiusFiltering` and `AllEntitiesWithRadiusFiltering`. The `KNearestEntitiesWithRadiusFiltering` filters out `kMaxAgentMapObservationsCount` nearest points within the `observationRadius` of the agents, while `AllEntitiesWithRadiusFiltering` runs a linear search in the same radius. Default: `KNearestEntitiesWithRadiusFiltering`.\n",
    "* `initOnlyValidAgentsAtFirstStep`: Controls if only agents valid at the first step are initialized into the simulator. Default: `true`.\n",
    "* `isStaticAgentControlled`: Controls if agents like parked vehicles that are already at their goals should be allowed to be controlled or set as static. Default: `false`.\n",
    "* `enableLidar`: Enables lidar observations.\n",
    "* `disableClassicalObs`: Disables setting `PartnerObservations` and `AgentMapObservations`. Generally used to speed up the simulator if lidar observations are enabled and the above observations are not used. Default: `false`.\n",
    "* `useWayMaxModel`: Sets if the WayMax dynamics model should be used. Default: `false`.\n",
    "\n",
    "#### Types of Objects\n",
    "\n",
    "* Road types:\n",
    "  - `RoadEdge`\n",
    "  - `RoadLine`\n",
    "  - `RoadLane`\n",
    "  - `CrossWalk`\n",
    "  - `SpeedBump`\n",
    "  - `StopSign`\n",
    "* Agent types:\n",
    "  - `Vehicle`\n",
    "  - `Pedestrian`\n",
    "  - `Cyclist`\n",
    "* `Padding` type: A special type used to pad entities in different worlds to ensure consistent output shapes.\n",
    "* `None` type: A special type marking entities as invalid and should not be considered for learning. This can arise if an entity is outside the `ObservationRadius` or if the entity collided and the collision behavior is set to `AgentRemoved`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "madrona",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
